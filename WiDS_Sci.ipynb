{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Women in Data Science - 2018 - Kaggle/Stanford Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Clean and Transform dataset for use with sklearn and tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/train.csv\", low_memory=False)\n",
    "df_test = pd.read_csv(\"Data/test.csv\", low_memory=False)\n",
    "df_dict = pd.read_csv('Data/WiDS data dictionary v2.csv')\n",
    "\n",
    "Y = df['is_female'] # Label\n",
    "X = df.drop(['is_female','train_id'], axis = 1)\n",
    "X = X.dropna(axis=1, how='all') # If all values are nans, drop col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_df(X):\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    ### Returns a dictionary object with dummified categorical variables and standardized numerical variables.\n",
    "\n",
    "    1. Check data dictionary to see which variables are numerical/categorical\n",
    "\n",
    "    - Get a tentative list of numerical and categorical variables:\n",
    "        \n",
    "        - For numerical variables, we first look at columns that have the values N/A or 99=DK. \n",
    "          Most variables that are numerical have this sort of 'value' in the data dictionary\n",
    "          To make sure, we treat anything with less than 10 distinct unique values as categorical.\n",
    "    \n",
    "    - For categorical vars, a starting point is all variables that do not have the dtype 'np.number'\n",
    "    - Final list of column names is stored as 'treat_as_num' and 'treat_as_cat'.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # For storage of column names that are categorical / numerical\n",
    "    treat_as_num = []\n",
    "    treat_as_cat = []\n",
    "\n",
    "    tentative_num = [i for i in df_dict[df_dict['Values'] == 'N/A\\n99=DK']['Column Name'].values if i in X.columns.values]\n",
    "    tentative_cat = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    treat_as_cat = treat_as_cat + tentative_cat[:]\n",
    "    \n",
    "    ### Columns that are in both the df and data dictionary.\n",
    "    cols_in_df_and_datadict = [i for i in X.columns.values if i in df_dict['Column Name'].values]\n",
    "\n",
    "    ### Columns that are NOT in the data dictionary but are found in the df.\n",
    "    cols_not_in_dict = [i for i in X.columns.values if i not in df_dict['Column Name'].values]\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    For columns in tentative_num, if column has less than ten categories, treat it as categorical\n",
    "    \n",
    "    '''\n",
    "\n",
    "    for col in tentative_num:\n",
    "        \n",
    "        num_of_categories = len(X[col].value_counts().keys())\n",
    "\n",
    "        if num_of_categories <= 10:\n",
    "            print('Change To Categorical : ', col, num_of_categories)\n",
    "            treat_as_cat.append(col)\n",
    "        else:\n",
    "            print('Keep As Numerical : ', col, num_of_categories)\n",
    "            treat_as_num.append(col)\n",
    "            \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    For columns not found in the dictionary, use the same crude rule - if number of distinct values <= 10,\n",
    "    treat as a categorical var else numerical\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for col in cols_not_in_dict:\n",
    "    \n",
    "        num_of_categories = len(X[col].value_counts().keys())\n",
    "    \n",
    "        if col in treat_as_cat:\n",
    "            pass\n",
    "        elif num_of_categories >= 10:\n",
    "            treat_as_num.append(col)\n",
    "        elif num_of_categories <= 10:\n",
    "            treat_as_cat.append(col)\n",
    "\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Every column that is in df and data dict but not in treat_as_num or treat_as_cat yet, treat them as categorical.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for col in cols_in_df_and_datadict:\n",
    "    \n",
    "        if col in treat_as_num:\n",
    "            pass\n",
    "\n",
    "        elif col in treat_as_cat:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            treat_as_cat.append(col)\n",
    "\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    Now we dummify the categorical variables and standardize the numerical vars\n",
    "    \n",
    "    '''\n",
    "\n",
    "    data_dict = {} \n",
    "    \n",
    "    for i in X.columns:\n",
    "\n",
    "        if i in treat_as_cat: # For every column, if categorical, convert to one hot encoding/dummy vars\n",
    "            \n",
    "            # Replace value of 99 [mostly refers to Don't Know in data] as np.nan\n",
    "            #X[i] = X[i].replace(to_replace=99,value=np.NaN)\n",
    "            X[i].fillna(X[i].mode(),inplace=True)\n",
    "\n",
    "            sub_dummy = pd.get_dummies(X[i],prefix=i,dummy_na=True)\n",
    "\n",
    "            for j in sub_dummy.columns.values: # For every column in dummified df, add to data_dict\n",
    "                data_dict[j] = sub_dummy[j].values\n",
    "\n",
    "        else: # If col is numerical, standardize, fill \n",
    "            \n",
    "            X[i].fillna(X[i].median(),inplace=True)\n",
    "            stdized_col = (X[i] - X[i].mean() )/ X[i].std()\n",
    "            data_dict[i] = stdized_col.values\n",
    "            \n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep As Numerical :  DG1 79\n",
      "Keep As Numerical :  DG8a 13\n",
      "Keep As Numerical :  DG8b 13\n",
      "Keep As Numerical :  DG8c 13\n",
      "Keep As Numerical :  DG9a 12\n",
      "Keep As Numerical :  DG9b 11\n",
      "Change To Categorical :  DG9c 8\n",
      "Keep As Numerical :  DL8 341\n",
      "Keep As Numerical :  DL11 15\n",
      "Keep As Numerical :  MT1 13\n",
      "Keep As Numerical :  MT6C 28\n",
      "Change To Categorical :  FF7_1 4\n",
      "Change To Categorical :  FF7_2 4\n",
      "Change To Categorical :  FF7_3 2\n",
      "Change To Categorical :  FF7_4 5\n",
      "Change To Categorical :  FF7_5 4\n",
      "Change To Categorical :  FF7_6 3\n",
      "Change To Categorical :  FF7_7 2\n",
      "Change To Categorical :  FF7_96 2\n",
      "Change To Categorical :  FF8_1 3\n",
      "Change To Categorical :  FF8_2 2\n",
      "Change To Categorical :  FF8_3 2\n",
      "Change To Categorical :  FF8_4 3\n",
      "Change To Categorical :  FF8_5 2\n",
      "Change To Categorical :  FF8_6 2\n",
      "Change To Categorical :  FF8_7 2\n",
      "Change To Categorical :  FF8_96 1\n",
      "Change To Categorical :  MM23 1\n",
      "Change To Categorical :  IFI18 9\n",
      "Keep As Numerical :  FB13 25\n",
      "Keep As Numerical :  FB14 14\n",
      "Keep As Numerical :  FB15 20\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame.from_dict(preprocess_df(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18255, 5643)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8485346480416325\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_features='auto',\n",
    "                             n_jobs=-1,\n",
    "                             random_state=1,\n",
    "                             criterion='gini',\n",
    "                             oob_score=True,\n",
    "                             bootstrap=True,\n",
    "                             )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [2, 7, 12, 18, 23, 28, 34, 39, 44, 50, None], 'min_samples_split': [2, 5, 10], 'bootstrap': [True, False], 'n_estimators': [10, 366, 722, 1079, 1435, 1792, 2148, 2505, 2861, 3217, 3574, 3930, 4287, 4643, 5000], 'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt', 'log2']}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 5000, num = 15)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model from our random search\n",
    "rf_best = rf_random.best_estimator_\n",
    "\n",
    "# For Accuracy\n",
    "y_pred = rf_best.predict(X_test)\n",
    "print('Accuracy :', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get probabilities for being female\n",
    "y_pred_prob = rf_best.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob] # Only get probabilities for is_female == 1 as per Kaggle website.\n",
    "print(\"AUC - ROC : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8761983018351137\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=5000,\n",
    "                                 learning_rate=0.5,\n",
    "                                 max_depth=None, \n",
    "                                 random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7cdc8351a038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                          learning_rate=1)\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_pred_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             return self._boost_discrete(iboost, X, y, sample_weight,\n\u001b[1;32m--> 477\u001b[1;33m                                         random_state)\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1000,\n",
    "                         learning_rate=1)\n",
    "\n",
    "bdt.fit(X_train, y_train)\n",
    "y_pred = bdt.predict(X_test)\n",
    "y_pred_prob = bdt.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob]\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"AUC - ROC : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': ['auto', 'sqrt', 'log2', None], 'min_samples_leaf': [1, 2, 4, 7, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 15, 30, 50, 100], 'max_depth': [2, 12, 23, 34, 45, 56, 66, 77, 88, 99, 110, None]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15,30,50, 100]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 7, 10, 20, 30, 50]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               }\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 327.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'max_features': ['auto', 'sqrt', 'log2', None], 'min_samples_leaf': [1, 2, 4, 7, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 15, 30, 50, 100], 'max_depth': [2, 12, 23, 34, 45, 56, 66, 77, 88, 99, 110, None]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "bdt_cv = DecisionTreeClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "bdt_random = RandomizedSearchCV(estimator = bdt_cv, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "bdt_random.fit(X_train, y_train)\n",
    "\n",
    "print(bdt_random.best_estimator_)\n",
    "print(bdt_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ada_best' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a9dcb81334fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mada_best\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Accuracy for AdaBoost with Decision Trees : '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ada_best' is not defined"
     ]
    }
   ],
   "source": [
    "### Now use decision tree with tuned hyperparameters as input to AdaBoost\n",
    "\n",
    "best_bdt = bdt_random.best_estimator_\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "ada = AdaBoostClassifier(best_bdt,\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1079,\n",
    "                         learning_rate=0.8276034482758621,\n",
    "                         )\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "print('Accuracy for AdaBoost with Decision Trees : ', accuracy_score(y_test,y_pred))\n",
    "\n",
    "y_pred_prob = ada.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob]\n",
    "\n",
    "print(\"AUC - ROC for AdaBoost with Decision Trees : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.0001, 0.03457931034482759, 0.06905862068965518, 0.10353793103448276, 0.13801724137931035, 0.17249655172413794, 0.2069758620689655, 0.2414551724137931, 0.2759344827586207, 0.3104137931034483, 0.3448931034482759, 0.3793724137931035, 0.41385172413793103, 0.44833103448275863, 0.4828103448275862, 0.5172896551724139, 0.5517689655172414, 0.586248275862069, 0.6207275862068966, 0.6552068965517241, 0.6896862068965518, 0.7241655172413793, 0.758644827586207, 0.7931241379310345, 0.8276034482758621, 0.8620827586206897, 0.8965620689655173, 0.9310413793103449, 0.9655206896551725, 1.0], 'n_estimators': [10, 366, 722, 1079, 1435, 1792, 2148, 2505, 2861, 3217, 3574, 3930, 4287, 4643, 5000], 'algorithm': ['SAMME', 'SAMME.R']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 5000, num = 15)]\n",
    "\n",
    "algorithm = ['SAMME','SAMME.R']\n",
    "# Maximum number of levels in tree\n",
    "learning_rate = [x for x in np.linspace(0.0001, 1, num = 30)]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'algorithm': algorithm,\n",
    "               'learning_rate': learning_rate,\n",
    "               }\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "ada_cv = AdaBoostClassifier(best_bdt)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "ada_random = RandomizedSearchCV(estimator = ada_cv, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "ada_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ada_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9156395508079979\n"
     ]
    }
   ],
   "source": [
    "# Create and fit an AdaBoosted decision tree\n",
    "ada_best = AdaBoostClassifier(best_bdt,\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1079,\n",
    "                         learning_rate=0.8276034482758621)\n",
    "\n",
    "ada_best.fit(X_train, y_train)\n",
    "y_pred = ada_best.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep As Numerical :  DG1 79\n",
      "Keep As Numerical :  DG8a 13\n",
      "Keep As Numerical :  DG8b 13\n",
      "Keep As Numerical :  DG8c 13\n",
      "Keep As Numerical :  DG9a 12\n",
      "Keep As Numerical :  DG9b 11\n",
      "Change To Categorical :  DG9c 8\n",
      "Keep As Numerical :  DL8 341\n",
      "Keep As Numerical :  DL11 15\n",
      "Keep As Numerical :  MT1 13\n",
      "Keep As Numerical :  MT6C 28\n",
      "Change To Categorical :  FF7_1 4\n",
      "Change To Categorical :  FF7_2 4\n",
      "Change To Categorical :  FF7_3 2\n",
      "Change To Categorical :  FF7_4 5\n",
      "Change To Categorical :  FF7_5 4\n",
      "Change To Categorical :  FF7_6 3\n",
      "Change To Categorical :  FF7_7 2\n",
      "Change To Categorical :  FF7_96 2\n",
      "Change To Categorical :  FF8_1 3\n",
      "Change To Categorical :  FF8_2 2\n",
      "Change To Categorical :  FF8_3 2\n",
      "Change To Categorical :  FF8_4 3\n",
      "Change To Categorical :  FF8_5 2\n",
      "Change To Categorical :  FF8_6 2\n",
      "Change To Categorical :  FF8_7 2\n",
      "Change To Categorical :  FF8_96 1\n",
      "Change To Categorical :  MM23 1\n",
      "Change To Categorical :  IFI18 9\n",
      "Keep As Numerical :  FB13 25\n",
      "Keep As Numerical :  FB14 14\n",
      "Keep As Numerical :  FB15 20\n"
     ]
    }
   ],
   "source": [
    "X_dict = preprocess_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop(['test_id'], axis = 1)\n",
    "df_test = df_test.dropna(axis=1, how='all') # If all values are nans, drop col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep As Numerical :  DG1 81\n",
      "Keep As Numerical :  DG8a 13\n",
      "Keep As Numerical :  DG8b 13\n",
      "Keep As Numerical :  DG8c 12\n",
      "Keep As Numerical :  DG9a 12\n",
      "Change To Categorical :  DG9b 8\n",
      "Change To Categorical :  DG9c 8\n",
      "Keep As Numerical :  DL8 391\n",
      "Keep As Numerical :  DL11 17\n",
      "Keep As Numerical :  MT1 12\n",
      "Keep As Numerical :  MT6C 37\n",
      "Change To Categorical :  FF7_1 4\n",
      "Change To Categorical :  FF7_2 4\n",
      "Change To Categorical :  FF7_3 3\n",
      "Change To Categorical :  FF7_4 5\n",
      "Change To Categorical :  FF7_5 4\n",
      "Change To Categorical :  FF7_6 4\n",
      "Change To Categorical :  FF7_7 4\n",
      "Change To Categorical :  FF7_96 2\n",
      "Change To Categorical :  FF8_1 4\n",
      "Change To Categorical :  FF8_2 2\n",
      "Change To Categorical :  FF8_3 2\n",
      "Change To Categorical :  FF8_4 4\n",
      "Change To Categorical :  FF8_5 2\n",
      "Change To Categorical :  FF8_6 3\n",
      "Change To Categorical :  FF8_7 2\n",
      "Change To Categorical :  FF8_96 1\n",
      "Change To Categorical :  MM23 2\n",
      "Change To Categorical :  IFI18 9\n",
      "Keep As Numerical :  FB13 28\n",
      "Keep As Numerical :  FB14 16\n",
      "Keep As Numerical :  FB15 23\n"
     ]
    }
   ],
   "source": [
    "X_test_dict = preprocess_df(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6138"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5697"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5012"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in X_test_dict.keys() if i in X_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cols = [i for i in X_test.columns.values if i in X.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4978"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame.from_dict(X_test_dict)[test_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = X[[i for i in X.columns.values if i in X_test.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(GradientBoostingClassifier(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=100,\n",
    "                         learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False),\n",
       "          learning_rate=0.0001, n_estimators=100, random_state=None)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9024924678170364"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bdt.score(X_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902492467817\n",
      "AUC - ROC :  0.965335239333\n"
     ]
    }
   ],
   "source": [
    "y_pred = bdt.predict(X_val)\n",
    "y_pred_prob = bdt.predict_proba(X_val)\n",
    "#y_pred_prob = [i[1] for i in y_pred_prob]\n",
    "\n",
    "print(accuracy_score(y_val,y_pred))\n",
    "print(\"AUC - ROC : \", roc_auc_score(y_val,y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = bdt.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = [i[1] for i in y_pred_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.51055672306395827,\n",
       " 0.5279217318851479,\n",
       " 0.51073951132230266,\n",
       " 0.53009818145722531,\n",
       " 0.52111866724396316,\n",
       " 0.53163238070407504,\n",
       " 0.52761230097812739,\n",
       " 0.52178365979945984,\n",
       " 0.52908001094361812,\n",
       " 0.52265108213839151,\n",
       " 0.53001564629510189,\n",
       " 0.52913201308872926,\n",
       " 0.53122351051101724,\n",
       " 0.53207359670893295,\n",
       " 0.52467566533373788,\n",
       " 0.53813620730280809,\n",
       " 0.52118953222184883,\n",
       " 0.53569683066123042,\n",
       " 0.52167674034867173,\n",
       " 0.53168839559749104,\n",
       " 0.52147782513311958,\n",
       " 0.53079409555421775,\n",
       " 0.52311467095011421,\n",
       " 0.52654546496633103,\n",
       " 0.52653123747547947,\n",
       " 0.52109326187137128,\n",
       " 0.52710216861596371,\n",
       " 0.52205852914333506,\n",
       " 0.52713377874734824,\n",
       " 0.53212925805389044,\n",
       " 0.53515304363759386,\n",
       " 0.521176498770064,\n",
       " 0.52804459182654107,\n",
       " 0.52857995266901825,\n",
       " 0.52458563613066667,\n",
       " 0.52506284839284101,\n",
       " 0.52851094333054971,\n",
       " 0.53365433430654385,\n",
       " 0.52165688724970938,\n",
       " 0.52180760080194322,\n",
       " 0.52408373365163408,\n",
       " 0.51797645522836633,\n",
       " 0.52453783599851234,\n",
       " 0.51562433748196801,\n",
       " 0.52906449121694221,\n",
       " 0.52006446104018289,\n",
       " 0.52548653983182769,\n",
       " 0.53022097115372313,\n",
       " 0.5285208629511513,\n",
       " 0.5187725639887778,\n",
       " 0.52470014401181908,\n",
       " 0.526759789371819,\n",
       " 0.52748039765564725,\n",
       " 0.52613107782960666,\n",
       " 0.52059208558930803,\n",
       " 0.5209877732325584,\n",
       " 0.5224658987117915,\n",
       " 0.52890801446258751,\n",
       " 0.5288681727898058,\n",
       " 0.52059819441709554,\n",
       " 0.53208293818086549,\n",
       " 0.52510015389299058,\n",
       " 0.52924919077248833,\n",
       " 0.52012683299558404,\n",
       " 0.52310172429317869,\n",
       " 0.52958540513411112,\n",
       " 0.52272300348838696,\n",
       " 0.51602374858547628,\n",
       " 0.5316992547639996,\n",
       " 0.52168780252769864,\n",
       " 0.52584706414200677,\n",
       " 0.53692256285395734,\n",
       " 0.52601303518243192,\n",
       " 0.52016419758350774,\n",
       " 0.52055214046853293,\n",
       " 0.52311862745654136,\n",
       " 0.51974868377443351,\n",
       " 0.52617450896519236,\n",
       " 0.51518811038573564,\n",
       " 0.51906554216309064,\n",
       " 0.53512960019631783,\n",
       " 0.51974243534993891,\n",
       " 0.51959482968559401,\n",
       " 0.52136147382232301,\n",
       " 0.52870698071332367,\n",
       " 0.51140722458113685,\n",
       " 0.51916140027460866,\n",
       " 0.52517703870057564,\n",
       " 0.5250703730021874,\n",
       " 0.52018133090000696,\n",
       " 0.51814135205285128,\n",
       " 0.52561002559006953,\n",
       " 0.521955491771118,\n",
       " 0.51963252274059568,\n",
       " 0.52418094836743978,\n",
       " 0.51953809163634623,\n",
       " 0.52455438458148673,\n",
       " 0.52944719129543527,\n",
       " 0.51953762020043792,\n",
       " 0.52017115656278479,\n",
       " 0.51860526985567368,\n",
       " 0.524566260479714,\n",
       " 0.51616135380861961,\n",
       " 0.52714861854225525,\n",
       " 0.53048917802541518,\n",
       " 0.52593769649639854,\n",
       " 0.52425211095930757,\n",
       " 0.51868510955620639,\n",
       " 0.52417106968847027,\n",
       " 0.52426638311911367,\n",
       " 0.52176963499081763,\n",
       " 0.52018211922266155,\n",
       " 0.51959239990773665,\n",
       " 0.51815785374999124,\n",
       " 0.53495167076865091,\n",
       " 0.52461403183029465,\n",
       " 0.52543118333960803,\n",
       " 0.52418662233758906,\n",
       " 0.52772632568696387,\n",
       " 0.51888799257458063,\n",
       " 0.52869458589866924,\n",
       " 0.52219869590304191,\n",
       " 0.52518800659670739,\n",
       " 0.52471404697175261,\n",
       " 0.52366683775481881,\n",
       " 0.51920634398526344,\n",
       " 0.53858998677369052,\n",
       " 0.52175381920722075,\n",
       " 0.52473017373272934,\n",
       " 0.51557322295006625,\n",
       " 0.51353505846362224,\n",
       " 0.52693316372728372,\n",
       " 0.52105364195556625,\n",
       " 0.50991729327154911,\n",
       " 0.52961392936065554,\n",
       " 0.5259540218967691,\n",
       " 0.51866970042198512,\n",
       " 0.52570051199126955,\n",
       " 0.51491517440734036,\n",
       " 0.52822950338678232,\n",
       " 0.51632115129202194,\n",
       " 0.52657321245121425,\n",
       " 0.52316365801661435,\n",
       " 0.52746361929330798,\n",
       " 0.53009030976358407,\n",
       " 0.52658041071792794,\n",
       " 0.52515251198758206,\n",
       " 0.51400277040077047,\n",
       " 0.51917325275486192,\n",
       " 0.52615740049960424,\n",
       " 0.52421353543599536,\n",
       " 0.52758808490519238,\n",
       " 0.52672957698103784,\n",
       " 0.53222584816186003,\n",
       " 0.52262973988591699,\n",
       " 0.53099774581917047,\n",
       " 0.52707046631989463,\n",
       " 0.52960767622837979,\n",
       " 0.52653120952197663,\n",
       " 0.51749137807722601,\n",
       " 0.52456805030400466,\n",
       " 0.51863866298697903,\n",
       " 0.52512721732621348,\n",
       " 0.52819920623079109,\n",
       " 0.53412078894775294,\n",
       " 0.51800227078309091,\n",
       " 0.52617865966392829,\n",
       " 0.52614314044678712,\n",
       " 0.51909814433268475,\n",
       " 0.52804789368419125,\n",
       " 0.52702829580764643,\n",
       " 0.5251330236610271,\n",
       " 0.52405669174120684,\n",
       " 0.52320470795675489,\n",
       " 0.52317585654150822,\n",
       " 0.51909331054210928,\n",
       " 0.5230649765556018,\n",
       " 0.52126316334541745,\n",
       " 0.53063853125595817,\n",
       " 0.53005104086262,\n",
       " 0.53665432427303861,\n",
       " 0.52621510012087458,\n",
       " 0.52369929281628125,\n",
       " 0.51616640337568398,\n",
       " 0.5240330362542257,\n",
       " 0.52707760374107027,\n",
       " 0.52626093114059602,\n",
       " 0.51751122378242509,\n",
       " 0.52507662614833706,\n",
       " 0.52656486932753155,\n",
       " 0.51857681006786738,\n",
       " 0.52010935689982607,\n",
       " 0.52458266979786872,\n",
       " 0.52922596693316104,\n",
       " 0.52571228580946583,\n",
       " 0.5242342778193424,\n",
       " 0.51291305578019941,\n",
       " 0.51919488124138913,\n",
       " 0.52411043799812318,\n",
       " 0.52563908206369059,\n",
       " 0.52549545264156661,\n",
       " 0.52253363589577417,\n",
       " 0.52572263249570006,\n",
       " 0.51310794567644091,\n",
       " 0.52557676416808186,\n",
       " 0.52511314232359085,\n",
       " 0.53060554039207863,\n",
       " 0.52270583765866752,\n",
       " 0.51602494005725119,\n",
       " 0.52369112651108018,\n",
       " 0.52613915374223219,\n",
       " 0.52611953379535714,\n",
       " 0.52700303624151601,\n",
       " 0.52463453314947273,\n",
       " 0.52970614581513542,\n",
       " 0.5285546097690873,\n",
       " 0.52366128060652684,\n",
       " 0.52320712763703858,\n",
       " 0.5124692747390468,\n",
       " 0.51827396383091606,\n",
       " 0.52453373670580583,\n",
       " 0.52459030043257993,\n",
       " 0.52209046140867699,\n",
       " 0.52659540903319701,\n",
       " 0.52166067036691954,\n",
       " 0.5241616748079625,\n",
       " 0.5260935768455427,\n",
       " 0.51641559559218841,\n",
       " 0.51942335891776392,\n",
       " 0.51803847093113797,\n",
       " 0.52508289327340441,\n",
       " 0.51999960461220174,\n",
       " 0.52261841209694049,\n",
       " 0.5231148277095895,\n",
       " 0.51750487974003168,\n",
       " 0.52153851049664501,\n",
       " 0.5335455860857744,\n",
       " 0.52825980932692296,\n",
       " 0.52198342380380736,\n",
       " 0.52111922942755096,\n",
       " 0.52275690983896561,\n",
       " 0.52003270910675037,\n",
       " 0.51918971970841021,\n",
       " 0.52419488087840649,\n",
       " 0.5301463093616231,\n",
       " 0.52167181312709254,\n",
       " 0.51854874693804964,\n",
       " 0.52211903445016916,\n",
       " 0.53750466018254039,\n",
       " 0.52901547562768036,\n",
       " 0.5210589569908447,\n",
       " 0.51505140047780473,\n",
       " 0.52104891892690564,\n",
       " 0.52773681718199206,\n",
       " 0.52543863315630746,\n",
       " 0.51900252640846944,\n",
       " 0.51720979247764243,\n",
       " 0.52306777282414318,\n",
       " 0.52355992301188514,\n",
       " 0.52021657527160137,\n",
       " 0.52458205086440512,\n",
       " 0.52698855749413909,\n",
       " 0.52761475238852273,\n",
       " 0.5320691846636546,\n",
       " 0.52068961834933569,\n",
       " 0.52055891519497866,\n",
       " 0.52017245662852296,\n",
       " 0.522122383085868,\n",
       " 0.5265714751880235,\n",
       " 0.52557344319493049,\n",
       " 0.5181640262559275,\n",
       " 0.52168050915362785,\n",
       " 0.52222495213034359,\n",
       " 0.5247036100586131,\n",
       " 0.52213538085222666,\n",
       " 0.52771746043242951,\n",
       " 0.52594840852038049,\n",
       " 0.53350126969170564,\n",
       " 0.53504851653507968,\n",
       " 0.52325150650499308,\n",
       " 0.51856106730546225,\n",
       " 0.5227085788410506,\n",
       " 0.53506667961604515,\n",
       " 0.52246244022634225,\n",
       " 0.52651957647138647,\n",
       " 0.5226507054821381,\n",
       " 0.52968284230010465,\n",
       " 0.52909553095844042,\n",
       " 0.52866573919286519,\n",
       " 0.52323289751244006,\n",
       " 0.52813235706867045,\n",
       " 0.5222138010429842,\n",
       " 0.52860089828727397,\n",
       " 0.52795453387004998,\n",
       " 0.52888087095307013,\n",
       " 0.53098939203245632,\n",
       " 0.51914273915635167,\n",
       " 0.51899793385381787,\n",
       " 0.52994859648668569,\n",
       " 0.52062646562100756,\n",
       " 0.5185338416871873,\n",
       " 0.53304753326076137,\n",
       " 0.5341064369003764,\n",
       " 0.52366914357216021,\n",
       " 0.5180041022946188,\n",
       " 0.51610407559988636,\n",
       " 0.52593155735932995,\n",
       " 0.52354442818974822,\n",
       " 0.52813700403024577,\n",
       " 0.52514859281391146,\n",
       " 0.51804956689914927,\n",
       " 0.52315690704094409,\n",
       " 0.52549848889096717,\n",
       " 0.52763803928568664,\n",
       " 0.52465937660008011,\n",
       " 0.52023750052652462,\n",
       " 0.52603603613971739,\n",
       " 0.52755532191623355,\n",
       " 0.52263097688788396,\n",
       " 0.52856125153906974,\n",
       " 0.52272804220326552,\n",
       " 0.5246693609812022,\n",
       " 0.52261595065657906,\n",
       " 0.52574942334931096,\n",
       " 0.52567652483254701,\n",
       " 0.52099607510869117,\n",
       " 0.52871060299286143,\n",
       " 0.52064572382916741,\n",
       " 0.52419293995168947,\n",
       " 0.53216056635169895,\n",
       " 0.51719382776651135,\n",
       " 0.5287000147870623,\n",
       " 0.53204138307891091,\n",
       " 0.52360259843497725,\n",
       " 0.52410982745359436,\n",
       " 0.52710255858861266,\n",
       " 0.52064538087443801,\n",
       " 0.52515031000552914,\n",
       " 0.52357511191519679,\n",
       " 0.51867190355036064,\n",
       " 0.52958341627643823,\n",
       " 0.52258249926456035,\n",
       " 0.52521425425371171,\n",
       " 0.53326320653575976,\n",
       " 0.52265246522011599,\n",
       " 0.52297641590382526,\n",
       " 0.52710845462907663,\n",
       " 0.52664201195427418,\n",
       " 0.51855021041341398,\n",
       " 0.52219276176601259,\n",
       " 0.51510832116692029,\n",
       " 0.52751395139385748,\n",
       " 0.52302528609626209,\n",
       " 0.52300085525081641,\n",
       " 0.52164175987962214,\n",
       " 0.53361492746837913,\n",
       " 0.52671333653613828,\n",
       " 0.52760813128645334,\n",
       " 0.51952991379043045,\n",
       " 0.51999442485399638,\n",
       " 0.52412777025550628,\n",
       " 0.521159810944817,\n",
       " 0.5301600541653656,\n",
       " 0.52170674698099506,\n",
       " 0.52484169551763393,\n",
       " 0.51977340580516407,\n",
       " 0.522666917754948,\n",
       " 0.52872170634022497,\n",
       " 0.5190704266335564,\n",
       " 0.52666555780516278,\n",
       " 0.52121583664270188,\n",
       " 0.52615622632340031,\n",
       " 0.52407577783428372,\n",
       " 0.52002522071175006,\n",
       " 0.51916319122410681,\n",
       " 0.52274687264136044,\n",
       " 0.53506051426228052,\n",
       " 0.53008865040145348,\n",
       " 0.51553818139435015,\n",
       " 0.51832767273432168,\n",
       " 0.53067283597615311,\n",
       " 0.52219503457880112,\n",
       " 0.53102441114949395,\n",
       " 0.52819262706359438,\n",
       " 0.51585098461369261,\n",
       " 0.52557757585791332,\n",
       " 0.52670866702079633,\n",
       " 0.5240085642721255,\n",
       " 0.53206602111756141,\n",
       " 0.5200796988542038,\n",
       " 0.52247991384229364,\n",
       " 0.52008113142553503,\n",
       " 0.5310243686006676,\n",
       " 0.53303706180843258,\n",
       " 0.52460071859891932,\n",
       " 0.52309645805250093,\n",
       " 0.52642028092390003,\n",
       " 0.52547829812209468,\n",
       " 0.52769081656341876,\n",
       " 0.5180924317168919,\n",
       " 0.52470607603660924,\n",
       " 0.52406312008826206,\n",
       " 0.53111397300432561,\n",
       " 0.5171151129061643,\n",
       " 0.52520635189753528,\n",
       " 0.52705044269772849,\n",
       " 0.51717970305470917,\n",
       " 0.52054195824009064,\n",
       " 0.52498757660925932,\n",
       " 0.51407466423758197,\n",
       " 0.52017594487662366,\n",
       " 0.52012126449108942,\n",
       " 0.52012838239341874,\n",
       " 0.52459559864910488,\n",
       " 0.53558206395735475,\n",
       " 0.52557637263906687,\n",
       " 0.52253474231720742,\n",
       " 0.51841928758082867,\n",
       " 0.53150812807769798,\n",
       " 0.51760431596706669,\n",
       " 0.52077933345459493,\n",
       " 0.52752638274976282,\n",
       " 0.51772438190732362,\n",
       " 0.51811344408445881,\n",
       " 0.52412879380255817,\n",
       " 0.52813495713047687,\n",
       " 0.52814210808600004,\n",
       " 0.52629057517051869,\n",
       " 0.52161858730037314,\n",
       " 0.51877062797982043,\n",
       " 0.5280339132059646,\n",
       " 0.52091403704606554,\n",
       " 0.52209726028504311,\n",
       " 0.52859616084415451,\n",
       " 0.51618397350718126,\n",
       " 0.52156639393454951,\n",
       " 0.52795227557557045,\n",
       " 0.51761163565157364,\n",
       " 0.53659450268888043,\n",
       " 0.52758533058141521,\n",
       " 0.52748732788237362,\n",
       " 0.52550566312727653,\n",
       " 0.52919066270473081,\n",
       " 0.52211111698004853,\n",
       " 0.52115400111605226,\n",
       " 0.53139967345690475,\n",
       " 0.52517868666505607,\n",
       " 0.52017488707066106,\n",
       " 0.5131370616186175,\n",
       " 0.52314501386741097,\n",
       " 0.52522558784168816,\n",
       " 0.5304510682686171,\n",
       " 0.5201835718622323,\n",
       " 0.52450779434553563,\n",
       " 0.52149239012482063,\n",
       " 0.52419697594665438,\n",
       " 0.52013756751416118,\n",
       " 0.51113632682806143,\n",
       " 0.52744472419349353,\n",
       " 0.5262022518131213,\n",
       " 0.52498646768731716,\n",
       " 0.52310351140976263,\n",
       " 0.52749997069413679,\n",
       " 0.52457271048804932,\n",
       " 0.52619903377371147,\n",
       " 0.52520757431580023,\n",
       " 0.5296094968492594,\n",
       " 0.5211663499920246,\n",
       " 0.53011534362111845,\n",
       " 0.52909398913606398,\n",
       " 0.51767231966002358,\n",
       " 0.52217009530388925,\n",
       " 0.51971659134338954,\n",
       " 0.52660663591940526,\n",
       " 0.51712963804997336,\n",
       " 0.51818022807464059,\n",
       " 0.5201332038863431,\n",
       " 0.52154684454312539,\n",
       " 0.5280846186810404,\n",
       " 0.52567251827513206,\n",
       " 0.53059201902457431,\n",
       " 0.52215150086232476,\n",
       " 0.52756749886163945,\n",
       " 0.51924844521015512,\n",
       " 0.52664985316771074,\n",
       " 0.52301425749998554,\n",
       " 0.5311971758291183,\n",
       " 0.53454844186999129,\n",
       " 0.5225756101230179,\n",
       " 0.51920411649218257,\n",
       " 0.52108296061258308,\n",
       " 0.52622103780435803,\n",
       " 0.52507631153157319,\n",
       " 0.53053133563906718,\n",
       " 0.52667157686869603,\n",
       " 0.52421254714479359,\n",
       " 0.52408364183885492,\n",
       " 0.51969303681153978,\n",
       " 0.52061909395578221,\n",
       " 0.52814741378313756,\n",
       " 0.52310694002953462,\n",
       " 0.52700578563655587,\n",
       " 0.53063139242383139,\n",
       " 0.5221792331502686,\n",
       " 0.52465205723016739,\n",
       " 0.51798699649563151,\n",
       " 0.52160723003316556,\n",
       " 0.52666395704471547,\n",
       " 0.52417729306012584,\n",
       " 0.51624431560198925,\n",
       " 0.52462095292643296,\n",
       " 0.51758170932563929,\n",
       " 0.52156574893622132,\n",
       " 0.52409463409694612,\n",
       " 0.51997272035300213,\n",
       " 0.52616387782258311,\n",
       " 0.52726156223818987,\n",
       " 0.52867406694731045,\n",
       " 0.52313027255989253,\n",
       " 0.51756669343611406,\n",
       " 0.52018512312190945,\n",
       " 0.52845760945578346,\n",
       " 0.53111708646194078,\n",
       " 0.52058429841355891,\n",
       " 0.53307376850782229,\n",
       " 0.52351527910786289,\n",
       " 0.52060838126756959,\n",
       " 0.53570762666881178,\n",
       " 0.52044150813491852,\n",
       " 0.52863604991570423,\n",
       " 0.52023718015069043,\n",
       " 0.52969408043924371,\n",
       " 0.51966992798528677,\n",
       " 0.530622063509533,\n",
       " 0.52223292631035301,\n",
       " 0.51807705936733439,\n",
       " 0.52160660616508403,\n",
       " 0.52221780921498329,\n",
       " 0.52468077157222026,\n",
       " 0.52044040014912363,\n",
       " 0.52849174685249622,\n",
       " 0.52657370431873962,\n",
       " 0.5265897262133189,\n",
       " 0.52320595723456809,\n",
       " 0.5349250067639959,\n",
       " 0.52909950369647984,\n",
       " 0.52617560089472626,\n",
       " 0.52396485780746738,\n",
       " 0.51749664836880394,\n",
       " 0.53077272911103557,\n",
       " 0.52857771924325392,\n",
       " 0.52374913935303447,\n",
       " 0.51918135052652403,\n",
       " 0.528302687845839,\n",
       " 0.53059827736070331,\n",
       " 0.52463169571192325,\n",
       " 0.52071600071164581,\n",
       " 0.52636169366218,\n",
       " 0.51457176726108023,\n",
       " 0.53258629859849449,\n",
       " 0.50904177634347614,\n",
       " 0.5254960149078568,\n",
       " 0.51719827495092363,\n",
       " 0.52518713464589417,\n",
       " 0.52861090931487908,\n",
       " 0.52560751899110381,\n",
       " 0.53261403466139268,\n",
       " 0.52676014275425487,\n",
       " 0.52608610164185987,\n",
       " 0.52204282644571898,\n",
       " 0.52704995232961338,\n",
       " 0.52420022791648313,\n",
       " 0.5246585096051567,\n",
       " 0.51715478884017352,\n",
       " 0.52711503842739871,\n",
       " 0.52532602491690883,\n",
       " 0.52873853054917708,\n",
       " 0.52669612806393884,\n",
       " 0.52522522528261406,\n",
       " 0.52867030238319368,\n",
       " 0.52142435107170348,\n",
       " 0.52964241842428617,\n",
       " 0.51971409316353878,\n",
       " 0.51652820058304338,\n",
       " 0.52603405563497485,\n",
       " 0.52140054200718211,\n",
       " 0.52467803158622439,\n",
       " 0.52155453160896748,\n",
       " 0.52418053709135315,\n",
       " 0.52563694838593977,\n",
       " 0.52257272900394536,\n",
       " 0.52607496191395275,\n",
       " 0.52202829202743661,\n",
       " 0.52244691671402965,\n",
       " 0.53097831534516016,\n",
       " 0.53316802725851542,\n",
       " 0.5213953756081493,\n",
       " 0.52453804171764595,\n",
       " 0.52868084594932574,\n",
       " 0.51696344023742147,\n",
       " 0.52046719064916847,\n",
       " 0.52063476257576224,\n",
       " 0.52496148717005331,\n",
       " 0.5259790098544771,\n",
       " 0.5334875504290677,\n",
       " 0.52530917093489971,\n",
       " 0.51352963877555691,\n",
       " 0.52504343493795325,\n",
       " 0.52202238219276653,\n",
       " 0.52351600580698809,\n",
       " 0.52860512606009091,\n",
       " 0.51712860075070233,\n",
       " 0.52485187861443805,\n",
       " 0.5327033381131937,\n",
       " 0.51398080686167957,\n",
       " 0.52856494621837435,\n",
       " 0.52409071929609774,\n",
       " 0.5307418272982779,\n",
       " 0.52304347470021439,\n",
       " 0.52015590082893626,\n",
       " 0.52415778526637136,\n",
       " 0.53165139402142503,\n",
       " 0.5179481127869181,\n",
       " 0.5315307098980625,\n",
       " 0.52312161644070476,\n",
       " 0.52441157101194935,\n",
       " 0.52536368764334618,\n",
       " 0.52761163470921824,\n",
       " 0.52714588445165067,\n",
       " 0.52662874366071488,\n",
       " 0.52811428022926732,\n",
       " 0.52566433154990178,\n",
       " 0.52730949543288674,\n",
       " 0.52201893258509746,\n",
       " 0.52295965632565578,\n",
       " 0.52314900862387537,\n",
       " 0.52172034556359248,\n",
       " 0.52265023891455009,\n",
       " 0.51956577974448959,\n",
       " 0.51775387705306875,\n",
       " 0.53003521861748693,\n",
       " 0.52166201866019213,\n",
       " 0.52290643020087579,\n",
       " 0.52279080809826983,\n",
       " 0.51715014016596261,\n",
       " 0.52470701680795495,\n",
       " 0.51388453773514631,\n",
       " 0.53361000097579991,\n",
       " 0.52876411978415561,\n",
       " 0.51112694817861726,\n",
       " 0.52316806727683263,\n",
       " 0.51968521534783318,\n",
       " 0.52321947279241043,\n",
       " 0.51745846421717689,\n",
       " 0.53181851894495769,\n",
       " 0.53466857526892664,\n",
       " 0.51608589604562105,\n",
       " 0.52052717845457996,\n",
       " 0.52769019759068647,\n",
       " 0.51795239844322749,\n",
       " 0.52568959104791879,\n",
       " 0.53807439822148162,\n",
       " 0.51660847128475851,\n",
       " 0.52497591450483161,\n",
       " 0.52363430933989519,\n",
       " 0.5291615250191386,\n",
       " 0.52513045698004024,\n",
       " 0.52432184660293768,\n",
       " 0.53051591790166219,\n",
       " 0.5227152892047785,\n",
       " 0.52007606829329711,\n",
       " 0.52314223525039116,\n",
       " 0.52260294955823383,\n",
       " 0.52559614070385696,\n",
       " 0.51880630384934656,\n",
       " 0.52501138475524722,\n",
       " 0.51957530988486034,\n",
       " 0.52366490418914025,\n",
       " 0.52319565678101398,\n",
       " 0.52804386323909291,\n",
       " 0.52664493695117243,\n",
       " 0.53159142365779488,\n",
       " 0.53004251497259758,\n",
       " 0.52823736473853911,\n",
       " 0.52846666405415998,\n",
       " 0.51650199279667308,\n",
       " 0.52522751841655146,\n",
       " 0.52015327452834093,\n",
       " 0.52168354945040774,\n",
       " 0.52272627375675684,\n",
       " 0.52969709430294643,\n",
       " 0.53341548584221321,\n",
       " 0.52108940434088658,\n",
       " 0.52755430044660601,\n",
       " 0.53913664546122553,\n",
       " 0.52666366724056335,\n",
       " 0.52959496735650502,\n",
       " 0.53115144779444068,\n",
       " 0.52197799647690657,\n",
       " 0.52991417317908096,\n",
       " 0.52311708552532699,\n",
       " 0.52958628476273573,\n",
       " 0.52061646743365886,\n",
       " 0.52646520598954016,\n",
       " 0.52018946262441035,\n",
       " 0.52408180742370103,\n",
       " 0.52865620327972795,\n",
       " 0.52755835405688678,\n",
       " 0.5291917713459583,\n",
       " 0.52448187989324191,\n",
       " 0.51745658693455931,\n",
       " 0.52889394349137375,\n",
       " 0.52609200557610414,\n",
       " 0.51965876238254771,\n",
       " 0.52036172344175746,\n",
       " 0.52292723816932607,\n",
       " 0.52473998400210553,\n",
       " 0.52359844283367007,\n",
       " 0.52119343036978605,\n",
       " 0.51715358873428807,\n",
       " 0.52816573416060164,\n",
       " 0.5226350140505408,\n",
       " 0.52677832188178031,\n",
       " 0.52012996128216282,\n",
       " 0.53555418569161739,\n",
       " 0.53661814434228061,\n",
       " 0.52311322839703556,\n",
       " 0.53408788240859961,\n",
       " 0.52206397988143216,\n",
       " 0.52523814054470652,\n",
       " 0.5231331631100784,\n",
       " 0.52774024954542298,\n",
       " 0.51918682523305237,\n",
       " 0.53103998233381144,\n",
       " 0.52458254910821134,\n",
       " 0.52661210227352062,\n",
       " 0.52617564932391869,\n",
       " 0.52573319288181775,\n",
       " 0.52299291752606425,\n",
       " 0.52015828665755826,\n",
       " 0.52163632297011686,\n",
       " 0.52172759386405931,\n",
       " 0.52044571511123017,\n",
       " 0.52417927221700744,\n",
       " 0.52414735411319091,\n",
       " 0.52514387484706671,\n",
       " 0.52366875944539015,\n",
       " 0.53016067009228496,\n",
       " 0.52617171221622927,\n",
       " 0.52499770694276782,\n",
       " 0.53762456840417505,\n",
       " 0.53109095115458493,\n",
       " 0.52463455733035624,\n",
       " 0.52457084928316533,\n",
       " 0.53159517237026066,\n",
       " 0.5329731509350758,\n",
       " 0.52069866916881014,\n",
       " 0.51666629279924092,\n",
       " 0.52418607884470958,\n",
       " 0.52462531704774562,\n",
       " 0.52606479169851039,\n",
       " 0.52266978900074956,\n",
       " 0.52111355193150588,\n",
       " 0.52559440434266169,\n",
       " 0.5236455001318997,\n",
       " 0.525042563943525,\n",
       " 0.53044332191608534,\n",
       " 0.524758752492961,\n",
       " 0.52201618982514064,\n",
       " 0.51865810480948749,\n",
       " 0.52163203015508786,\n",
       " 0.51899227675946968,\n",
       " 0.51982286662374622,\n",
       " 0.52704719387439813,\n",
       " 0.52055820738828507,\n",
       " 0.52759662198488055,\n",
       " 0.5240625204585212,\n",
       " 0.52147288104463385,\n",
       " 0.5144600827228144,\n",
       " 0.53024101608472485,\n",
       " 0.51817624848539012,\n",
       " 0.52767067000390711,\n",
       " 0.52617941125721213,\n",
       " 0.52318048861966726,\n",
       " 0.51956272495381528,\n",
       " 0.52952467411344017,\n",
       " 0.53013248672373792,\n",
       " 0.52174466786585483,\n",
       " 0.52466351693372637,\n",
       " 0.53007967151178115,\n",
       " 0.52415067141925686,\n",
       " 0.52508281359742592,\n",
       " 0.5231574053850625,\n",
       " 0.52903212621440332,\n",
       " 0.52167690706637559,\n",
       " 0.52309325237427107,\n",
       " 0.52674625968732636,\n",
       " 0.51646186866895127,\n",
       " 0.52257410921305847,\n",
       " 0.52041959014144534,\n",
       " 0.52970261091624382,\n",
       " 0.53712167592485571,\n",
       " 0.52441101187536787,\n",
       " 0.5305628201105268,\n",
       " 0.52509809903759064,\n",
       " 0.53218223673618803,\n",
       " 0.52967491075419848,\n",
       " 0.53005780999693186,\n",
       " 0.51854514711231359,\n",
       " 0.52409133804977126,\n",
       " 0.5331041359634775,\n",
       " 0.52508221524987753,\n",
       " 0.52540919496349714,\n",
       " 0.5231654084481373,\n",
       " 0.52317822858685192,\n",
       " 0.52524934461747519,\n",
       " 0.52216032563819237,\n",
       " 0.53004317140475998,\n",
       " 0.52805824093491005,\n",
       " 0.52223036704042947,\n",
       " 0.53003181958168122,\n",
       " 0.52419384993029638,\n",
       " 0.52546363713341582,\n",
       " 0.53188984745661672,\n",
       " 0.52412329624910325,\n",
       " 0.52660115685216025,\n",
       " 0.51946862229749091,\n",
       " 0.51967959718513135,\n",
       " 0.53022829604610178,\n",
       " 0.51366329012089296,\n",
       " 0.51955011804363904,\n",
       " 0.51461782567091208,\n",
       " 0.52349838645263536,\n",
       " 0.5291558245963689,\n",
       " 0.5254930592201621,\n",
       " 0.52270296867950117,\n",
       " 0.52175181534190695,\n",
       " 0.51916210657865602,\n",
       " 0.52201756885142214,\n",
       " 0.52568697046555202,\n",
       " 0.54208289071802807,\n",
       " 0.52173670403474048,\n",
       " 0.52950210131950382,\n",
       " 0.5276751314525806,\n",
       " 0.52713172155086452,\n",
       " 0.52067291908352664,\n",
       " 0.53000553078312596,\n",
       " 0.52710501572753554,\n",
       " 0.52308476460650355,\n",
       " 0.52917235658628159,\n",
       " 0.52835087769879996,\n",
       " 0.51855794423490942,\n",
       " 0.52270303876536928,\n",
       " 0.51759481282329745,\n",
       " 0.5306374301231882,\n",
       " 0.51865475076083289,\n",
       " 0.52270418544745145,\n",
       " 0.51955832902514576,\n",
       " 0.52012229470628635,\n",
       " 0.52612819154959412,\n",
       " 0.52117031906550038,\n",
       " 0.52269761227682432,\n",
       " 0.52067626542125145,\n",
       " 0.52255531485341156,\n",
       " 0.52611002391662953,\n",
       " 0.52615029850691231,\n",
       " 0.52755336657488161,\n",
       " 0.5276542116895353,\n",
       " 0.52730328307834495,\n",
       " 0.52719526311803167,\n",
       " 0.5291582189537215,\n",
       " 0.52570314220022141,\n",
       " 0.52566067381161607,\n",
       " 0.51726011157404561,\n",
       " 0.53354843242135319,\n",
       " 0.52715290988452201,\n",
       " 0.52803158079392409,\n",
       " 0.51923094748804632,\n",
       " 0.52269653644105474,\n",
       " 0.52651347115871894,\n",
       " 0.53108740571089341,\n",
       " 0.52757937124187182,\n",
       " 0.52152701151817416,\n",
       " 0.52574856689058025,\n",
       " 0.52404064247687865,\n",
       " 0.5185335086817725,\n",
       " 0.52570856016627632,\n",
       " 0.52218717490144118,\n",
       " 0.51921139883416634,\n",
       " 0.52245338655181239,\n",
       " 0.52761117679716163,\n",
       " 0.52425574314964418,\n",
       " 0.52521669149272521,\n",
       " 0.52421130600171617,\n",
       " 0.52420775083559612,\n",
       " 0.5287296629549596,\n",
       " 0.52471153710489982,\n",
       " 0.52267733488190993,\n",
       " 0.52413193065998176,\n",
       " 0.53061529731685908,\n",
       " 0.52562764407457652,\n",
       " 0.5287944070015671,\n",
       " 0.51756597157792072,\n",
       " 0.51845063184810336,\n",
       " 0.5214120880363563,\n",
       " 0.53057227570188359,\n",
       " 0.52572541092267155,\n",
       " 0.51369973175388517,\n",
       " 0.52140527649933632,\n",
       " 0.53439735918065312,\n",
       " 0.52006335097483614,\n",
       " 0.53502407600104829,\n",
       " 0.51907170432129024,\n",
       " 0.51957307839377331,\n",
       " 0.53000582975872534,\n",
       " 0.52539357414963883,\n",
       " 0.52318870496611636,\n",
       " 0.52332206075348109,\n",
       " 0.52321415978649766,\n",
       " 0.52657098234665223,\n",
       " 0.51625591112176894,\n",
       " 0.52584483002881188,\n",
       " 0.52018629193280341,\n",
       " 0.52798087604598976,\n",
       " 0.52901379365628753,\n",
       " 0.52458942851816448,\n",
       " 0.52464892648643013,\n",
       " 0.52842247236755413,\n",
       " 0.5135924513794653,\n",
       " 0.52215013926384335,\n",
       " 0.52539023102225013,\n",
       " 0.52699939886200919,\n",
       " 0.52116001284160374,\n",
       " 0.5251885285929111,\n",
       " 0.5225860543795966,\n",
       " 0.53019784980232421,\n",
       " 0.52271174776587925,\n",
       " 0.52761486569161165,\n",
       " 0.52549463499229976,\n",
       " 0.52165716540381923,\n",
       " 0.52860966633837125,\n",
       " 0.51759417917212591,\n",
       " 0.52413606165553217,\n",
       " 0.52972138268405267,\n",
       " 0.52709242264599543,\n",
       " 0.53213472900436698,\n",
       " 0.52015551827082007,\n",
       " 0.52113652215701078,\n",
       " 0.53253555848157752,\n",
       " 0.52011560536045054,\n",
       " 0.53014765368838745,\n",
       " 0.52875715372688137,\n",
       " 0.52803699434117379,\n",
       " 0.51918269872845157,\n",
       " 0.52917157212575128,\n",
       " 0.52850068099573266,\n",
       " 0.5246955396811297,\n",
       " 0.52602143161725534,\n",
       " 0.51248721931277552,\n",
       " 0.52567892011868855,\n",
       " 0.52132992758851537,\n",
       " 0.52068542910406135,\n",
       " 0.51854080637124389,\n",
       " 0.52409745296297927,\n",
       " 0.52024396197614653,\n",
       " 0.52024387344833378,\n",
       " 0.52470888093621393,\n",
       " 0.52349502036388951,\n",
       " 0.53254836428783059,\n",
       " 0.5172598291666527,\n",
       " 0.52172239609134041,\n",
       " 0.52656187213273797,\n",
       " 0.51769226134551538,\n",
       " 0.52117351347155538,\n",
       " 0.53001821466378751,\n",
       " 0.51954141412479671,\n",
       " 0.5276911966931952,\n",
       " 0.52767506371159323,\n",
       " 0.53002247370514821,\n",
       " 0.52471930160493541,\n",
       " 0.52262259506004771,\n",
       " 0.51997027368538418,\n",
       " 0.53112785633498139,\n",
       " 0.5186093661938812,\n",
       " 0.52011097896656144,\n",
       " 0.51727914843916123,\n",
       " 0.52022382494668351,\n",
       " 0.51713696320409575,\n",
       " 0.52219841582272886,\n",
       " 0.52864565649478501,\n",
       " 0.52119624611745163,\n",
       " 0.52672771726261147,\n",
       " 0.5196531637987567,\n",
       " 0.53403040619573328,\n",
       " 0.52809001566138836,\n",
       " 0.52206292931323539,\n",
       " 0.52467148456471058,\n",
       " 0.51913887581020168,\n",
       " 0.52211191425974834,\n",
       " 0.52113120097111265,\n",
       " ...]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_pred_prob,columns=['is_female'],index=df_test['test_id']).to_csv('submission_trial1.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script config_template.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
