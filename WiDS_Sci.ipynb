{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Women in Data Science - 2018 - Kaggle/Stanford Uni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Clean and Transform dataset for use with sklearn and tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/train.csv\", low_memory=False)\n",
    "df_test = pd.read_csv(\"Data/test.csv\", low_memory=False)\n",
    "df_dict = pd.read_csv('Data/WiDS data dictionary v2.csv')\n",
    "\n",
    "Y = df['is_female'] # Label\n",
    "X = df.drop(['is_female','train_id'], axis = 1)\n",
    "X = X.dropna(axis=1, how='all') # If all values are nans, drop col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(X):\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    ### Returns a dictionary object with dummified categorical variables and standardized numerical variables.\n",
    "\n",
    "    1. Check data dictionary to see which variables are numerical/categorical\n",
    "\n",
    "    - Get a tentative list of numerical and categorical variables:\n",
    "        \n",
    "        - For numerical variables, we first look at columns that have the values N/A or 99=DK. \n",
    "          Most variables that are numerical have this sort of 'value' in the data dictionary\n",
    "          To make sure, we treat anything with less than 10 distinct unique values as categorical.\n",
    "    \n",
    "    - For categorical vars, a starting point is all variables that do not have the dtype 'np.number'\n",
    "    - Final list of column names is stored as 'treat_as_num' and 'treat_as_cat'.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # For storage of column names that are categorical / numerical\n",
    "    treat_as_num = []\n",
    "    treat_as_cat = []\n",
    "\n",
    "    tentative_num = [i for i in df_dict[df_dict['Values'] == 'N/A\\n99=DK']['Column Name'].values if i in X.columns.values]\n",
    "    tentative_cat = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "    treat_as_cat = treat_as_cat + tentative_cat[:]\n",
    "    \n",
    "    ### Columns that are in both the df and data dictionary.\n",
    "    cols_in_df_and_datadict = [i for i in X.columns.values if i in df_dict['Column Name'].values]\n",
    "\n",
    "    ### Columns that are NOT in the data dictionary but are found in the df.\n",
    "    cols_not_in_dict = [i for i in X.columns.values if i not in df_dict['Column Name'].values]\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    For columns in tentative_num, if column has less than ten categories, treat it as categorical\n",
    "    \n",
    "    '''\n",
    "\n",
    "    for col in tentative_num:\n",
    "        \n",
    "        num_of_categories = len(X[col].value_counts().keys())\n",
    "\n",
    "        if num_of_categories <= 10:\n",
    "            print('Change To Categorical : ', col, num_of_categories)\n",
    "            treat_as_cat.append(col)\n",
    "        else:\n",
    "            print('Keep As Numerical : ', col, num_of_categories)\n",
    "            treat_as_num.append(col)\n",
    "            \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    For columns not found in the dictionary, use the same crude rule - if number of distinct values <= 10,\n",
    "    treat as a categorical var else numerical\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for col in cols_not_in_dict:\n",
    "    \n",
    "        num_of_categories = len(X[col].value_counts().keys())\n",
    "    \n",
    "        if col in treat_as_cat:\n",
    "            pass\n",
    "        elif num_of_categories >= 10:\n",
    "            treat_as_num.append(col)\n",
    "        elif num_of_categories <= 10:\n",
    "            treat_as_cat.append(col)\n",
    "\n",
    "    ####\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Every column that is in df and data dict but not in treat_as_num or treat_as_cat yet, treat them as categorical.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for col in cols_in_df_and_datadict:\n",
    "    \n",
    "        if col in treat_as_num:\n",
    "            pass\n",
    "\n",
    "        elif col in treat_as_cat:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            treat_as_cat.append(col)\n",
    "\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    '''\n",
    "    Now we dummify the categorical variables and standardize the numerical vars\n",
    "    \n",
    "    '''\n",
    "\n",
    "    data_dict = {} \n",
    "    \n",
    "    for i in X.columns:\n",
    "\n",
    "        if i in treat_as_cat: # For every column, if categorical, convert to one hot encoding/dummy vars\n",
    "            \n",
    "            # Replace value of 99 [mostly refers to Don't Know in data] as np.nan\n",
    "            X[i] = X[i].replace(to_replace=99,value=np.NaN) \n",
    "\n",
    "            sub_dummy = pd.get_dummies(X[i],prefix=i,dummy_na=True)\n",
    "\n",
    "            for j in sub_dummy.columns.values: # For every column in dummified df, add to data_dict\n",
    "                data_dict[j] = sub_dummy[j].values\n",
    "\n",
    "        else: # If col is numerical, standardize, fill \n",
    "            \n",
    "            X[i].fillna(X[i].median(),inplace=True)\n",
    "            stdized_col = (X[i] - X[i].mean() )/ X[i].std()\n",
    "            data_dict[i] = stdized_col.values\n",
    "            \n",
    "            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep As Numerical :  DG1 79\n",
      "Keep As Numerical :  DG8a 13\n",
      "Keep As Numerical :  DG8b 13\n",
      "Keep As Numerical :  DG8c 13\n",
      "Keep As Numerical :  DG9a 12\n",
      "Keep As Numerical :  DG9b 11\n",
      "Change To Categorical :  DG9c 8\n",
      "Keep As Numerical :  DL8 341\n",
      "Keep As Numerical :  DL11 15\n",
      "Keep As Numerical :  MT1 13\n",
      "Keep As Numerical :  MT6C 28\n",
      "Change To Categorical :  FF7_1 4\n",
      "Change To Categorical :  FF7_2 4\n",
      "Change To Categorical :  FF7_3 2\n",
      "Change To Categorical :  FF7_4 5\n",
      "Change To Categorical :  FF7_5 4\n",
      "Change To Categorical :  FF7_6 3\n",
      "Change To Categorical :  FF7_7 2\n",
      "Change To Categorical :  FF7_96 2\n",
      "Change To Categorical :  FF8_1 3\n",
      "Change To Categorical :  FF8_2 2\n",
      "Change To Categorical :  FF8_3 2\n",
      "Change To Categorical :  FF8_4 3\n",
      "Change To Categorical :  FF8_5 2\n",
      "Change To Categorical :  FF8_6 2\n",
      "Change To Categorical :  FF8_7 2\n",
      "Change To Categorical :  FF8_96 1\n",
      "Change To Categorical :  MM23 1\n",
      "Change To Categorical :  IFI18 9\n",
      "Keep As Numerical :  FB13 25\n",
      "Keep As Numerical :  FB14 14\n",
      "Keep As Numerical :  FB15 20\n"
     ]
    }
   ],
   "source": [
    "X = pd.DataFrame.from_dict(preprocess_df(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18255, 5643)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "C:\\Users\\Saifi\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8485346480416325\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,\n",
    "                             max_features='auto',\n",
    "                             n_jobs=-1,\n",
    "                             random_state=1,\n",
    "                             criterion='gini',\n",
    "                             oob_score=True,\n",
    "                             bootstrap=True,\n",
    "                             )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [2, 7, 12, 18, 23, 28, 34, 39, 44, 50, None], 'min_samples_split': [2, 5, 10], 'bootstrap': [True, False], 'n_estimators': [10, 366, 722, 1079, 1435, 1792, 2148, 2505, 2861, 3217, 3574, 3930, 4287, 4643, 5000], 'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt', 'log2']}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 5000, num = 15)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "# Best model from our random search\n",
    "rf_best = rf_random.best_estimator_\n",
    "\n",
    "# For Accuracy\n",
    "y_pred = rf_best.predict(X_test)\n",
    "print('Accuracy :', accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Get probabilities for being female\n",
    "y_pred_prob = rf_best.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob] # Only get probabilities for is_female == 1 as per Kaggle website.\n",
    "print(\"AUC - ROC : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8761983018351137\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=5000,\n",
    "                                 learning_rate=0.5,\n",
    "                                 max_depth=None, \n",
    "                                 random_state=0)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "#clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7cdc8351a038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                          learning_rate=1)\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0my_pred_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m             return self._boost_discrete(iboost, X, y, sample_weight,\n\u001b[1;32m--> 477\u001b[1;33m                                         random_state)\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1000,\n",
    "                         learning_rate=1)\n",
    "\n",
    "bdt.fit(X_train, y_train)\n",
    "y_pred = bdt.predict(X_test)\n",
    "y_pred_prob = bdt.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob]\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(\"AUC - ROC : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': ['auto', 'sqrt', 'log2', None], 'min_samples_leaf': [1, 2, 4, 7, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 15, 30, 50, 100], 'max_depth': [2, 12, 23, 34, 45, 56, 66, 77, 88, 99, 110, None]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt','log2',None]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10, 15,30,50, 100]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 7, 10, 20, 30, 50]\n",
    "\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               }\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 11.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed: 327.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score='raise',\n",
       "          estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          fit_params=None, iid=True, n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'max_features': ['auto', 'sqrt', 'log2', None], 'min_samples_leaf': [1, 2, 4, 7, 10, 20, 30, 50], 'min_samples_split': [2, 5, 10, 15, 30, 50, 100], 'max_depth': [2, 12, 23, 34, 45, 56, 66, 77, 88, 99, 110, None]},\n",
       "          pre_dispatch='2*n_jobs', random_state=1, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "bdt_cv = DecisionTreeClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "bdt_random = RandomizedSearchCV(estimator = bdt_cv, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 100, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "bdt_random.fit(X_train, y_train)\n",
    "\n",
    "print(bdt_random.best_estimator_)\n",
    "print(bdt_random.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now use decision tree with tuned hyperparameters as input to AdaBoost\n",
    "\n",
    "best_bdt = bdt_random.best_estimator_\n",
    "\n",
    "# Create and fit an AdaBoosted decision tree\n",
    "ada = AdaBoostClassifier(best_bdt,\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1079,\n",
    "                         learning_rate=0.8276034482758621,\n",
    "                         )\n",
    "\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "print('Accuracy for AdaBoost with Decision Trees : ', accuracy_score(y_test,y_pred))\n",
    "\n",
    "y_pred_prob = ada.predict_proba(X_test)\n",
    "y_pred_prob = [i[1] for i in y_pred_prob]\n",
    "\n",
    "print(\"AUC - ROC for AdaBoost with Decision Trees : \", roc_auc_score(y_test,y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.0001, 0.03457931034482759, 0.06905862068965518, 0.10353793103448276, 0.13801724137931035, 0.17249655172413794, 0.2069758620689655, 0.2414551724137931, 0.2759344827586207, 0.3104137931034483, 0.3448931034482759, 0.3793724137931035, 0.41385172413793103, 0.44833103448275863, 0.4828103448275862, 0.5172896551724139, 0.5517689655172414, 0.586248275862069, 0.6207275862068966, 0.6552068965517241, 0.6896862068965518, 0.7241655172413793, 0.758644827586207, 0.7931241379310345, 0.8276034482758621, 0.8620827586206897, 0.8965620689655173, 0.9310413793103449, 0.9655206896551725, 1.0], 'n_estimators': [10, 366, 722, 1079, 1435, 1792, 2148, 2505, 2861, 3217, 3574, 3930, 4287, 4643, 5000], 'algorithm': ['SAMME', 'SAMME.R']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 5000, num = 15)]\n",
    "\n",
    "algorithm = ['SAMME','SAMME.R']\n",
    "# Maximum number of levels in tree\n",
    "learning_rate = [x for x in np.linspace(0.0001, 1, num = 30)]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'algorithm': algorithm,\n",
    "               'learning_rate': learning_rate,\n",
    "               }\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "ada_cv = AdaBoostClassifier(best_bdt)\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "ada_random = RandomizedSearchCV(estimator = ada_cv, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, cv = 3, verbose=3, random_state=1, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "ada_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9156395508079979\n"
     ]
    }
   ],
   "source": [
    "# Create and fit an AdaBoosted decision tree\n",
    "ada_best = AdaBoostClassifier(best_bdt,\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1079,\n",
    "                         learning_rate=0.8276034482758621)\n",
    "\n",
    "ada_best.fit(X_train, y_train)\n",
    "y_pred = ada_best.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script config_template.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
